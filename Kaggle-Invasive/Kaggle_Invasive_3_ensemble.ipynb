{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/c/invasive-species-monitoring/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "HOMEPATH = \"/home/ubuntu/fastai/\"\n",
    "DATA_HOME_DIR = HOMEPATH + \"data/invasive/\"\n",
    "\n",
    "validFract = 0.3\n",
    "sampleFract = 0.2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('os.getcwd:', '/home/ubuntu/fastai')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5110)\n",
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n"
     ]
    }
   ],
   "source": [
    "os.chdir(HOMEPATH)\n",
    "print (\"os.getcwd:\", os.getcwd())\n",
    "# Rather than importing everything manually, we'll make things easy\n",
    "#   and load them all in utils.py, and just import them from there.\n",
    "import utils;\n",
    "from utils import *\n",
    "from shutil import copyfile\n",
    "\n",
    "from __future__ import division, print_function\n",
    "\n",
    "from vgg16 import Vgg16\n",
    "from keras.models import load_model\n",
    "\n",
    "from theano.sandbox import cuda\n",
    "cuda.use('gpu0')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model training configuration\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN  = DATA_HOME_DIR + \"train/\"\n",
    "VALID  = DATA_HOME_DIR + \"valid/\"\n",
    "TEST   = DATA_HOME_DIR + \"test/\"\n",
    "RESULTS = DATA_HOME_DIR + \"results/\"\n",
    "SAMPLE =  DATA_HOME_DIR + \"sample/\"\n",
    "SAMPLE_TRAIN = SAMPLE + \"train/\"\n",
    "SAMPLE_VALID = SAMPLE + \"valid/\"\n",
    "SAMPLE_TEST = SAMPLE + \"test/\"\n",
    "SAMPLE_RESULTS = SAMPLE + \"results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKING_DATA: /home/ubuntu/fastai/data/invasive/sample/\n",
      "WORKING_TEST: /home/ubuntu/fastai/data/invasive/sample/test/\n",
      "WORKING_TRAIN: /home/ubuntu/fastai/data/invasive/sample/train/\n",
      "WORKING_VALID: /home/ubuntu/fastai/data/invasive/sample/valid/\n",
      "WORKING_RESULTS: /home/ubuntu/fastai/data/invasive/sample/results/\n",
      "model_path: /home/ubuntu/fastai/data/invasive/sample/results/\n"
     ]
    }
   ],
   "source": [
    "#change this as appropriate between smaller sample data set(SAMPLE) and production data(DATA_HOME_DIR)\n",
    "WORKING_DATA = SAMPLE\n",
    "#WORKING_DATA = DATA_HOME_DIR \n",
    "#------------------future me : swap between above options. do not change below--------------\n",
    "WORKING_TEST = WORKING_DATA + \"test/\" #We use all the test data\n",
    "WORKING_TRAIN = WORKING_DATA + \"train/\"\n",
    "WORKING_VALID = WORKING_DATA + \"valid/\"\n",
    "WORKING_RESULTS = WORKING_DATA + \"results/\"\n",
    "\n",
    "print (\"WORKING_DATA:\", WORKING_DATA)\n",
    "print (\"WORKING_TEST:\", WORKING_TEST)\n",
    "print (\"WORKING_TRAIN:\", WORKING_TRAIN)\n",
    "print (\"WORKING_VALID:\", WORKING_VALID)\n",
    "print (\"WORKING_RESULTS:\", WORKING_RESULTS)\n",
    "model_path = WORKING_RESULTS\n",
    "print (\"model_path:\", model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/fastai/data/invasive/\n",
      "os.getcwd: /home/ubuntu/fastai/data/invasive\n",
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34mresults\u001b[00m\r\n",
      "├── \u001b[01;34msample\u001b[00m\r\n",
      "│   ├── \u001b[01;34mresults\u001b[00m\r\n",
      "│   ├── \u001b[01;34mtest\u001b[00m\r\n",
      "│   │   └── \u001b[01;34munknown\u001b[00m\r\n",
      "│   ├── \u001b[01;34mtrain\u001b[00m\r\n",
      "│   │   ├── \u001b[01;34mharmless\u001b[00m\r\n",
      "│   │   └── \u001b[01;34minvasive\u001b[00m\r\n",
      "│   └── \u001b[01;34mvalid\u001b[00m\r\n",
      "│       ├── \u001b[01;34mharmless\u001b[00m\r\n",
      "│       └── \u001b[01;34minvasive\u001b[00m\r\n",
      "├── \u001b[01;34mtest\u001b[00m\r\n",
      "│   └── \u001b[01;34munknown\u001b[00m\r\n",
      "├── \u001b[01;34mtrain\u001b[00m\r\n",
      "│   ├── \u001b[01;34mharmless\u001b[00m\r\n",
      "│   └── \u001b[01;34minvasive\u001b[00m\r\n",
      "└── \u001b[01;34mvalid\u001b[00m\r\n",
      "    ├── \u001b[01;34mharmless\u001b[00m\r\n",
      "    └── \u001b[01;34minvasive\u001b[00m\r\n",
      "\r\n",
      "19 directories\r\n"
     ]
    }
   ],
   "source": [
    "# traverse root directory, and list directories as dirs and files as files\n",
    "print (DATA_HOME_DIR)\n",
    "import os\n",
    "next(os.walk(DATA_HOME_DIR))[1]\n",
    "os.chdir(DATA_HOME_DIR)\n",
    "print (\"os.getcwd:\", os.getcwd())\n",
    "!tree -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKING_TRAIN: /home/ubuntu/fastai/data/invasive/sample/train/\n",
      "WORKING_VALID: /home/ubuntu/fastai/data/invasive/sample/valid/\n",
      "Found 320 images belonging to 2 classes.\n",
      "Found 136 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "print (\"WORKING_TRAIN:\", WORKING_TRAIN)\n",
    "print (\"WORKING_VALID:\", WORKING_VALID)\n",
    "batches = get_batches(WORKING_TRAIN, shuffle=False, batch_size=batch_size)\n",
    "val_batches = get_batches(WORKING_VALID, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 320 images belonging to 2 classes.\n",
      "Found 136 images belonging to 2 classes.\n",
      "Found 306 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "(val_classes, trn_classes, val_labels, trn_labels, val_filenames, filenames, test_filenames) = get_classes(WORKING_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_classes: <type 'numpy.ndarray'> (136,)\n",
      "trn_classes: <type 'numpy.ndarray'> (320,)\n",
      "val_labels: <type 'numpy.ndarray'> (136, 2)\n",
      "trn_labels: <type 'numpy.ndarray'> (320, 2)\n",
      "val_filenames: <type 'list'> 136\n",
      "filenames: <type 'list'> 320\n",
      "test_filenames: <type 'list'> 306\n"
     ]
    }
   ],
   "source": [
    "print (\"val_classes:\", type(val_classes), val_classes.shape)\n",
    "print (\"trn_classes:\", type(trn_classes), trn_classes.shape)\n",
    "print (\"val_labels:\", type(val_labels), val_labels.shape)\n",
    "print (\"trn_labels:\", type(trn_labels), trn_labels.shape)\n",
    "print (\"val_filenames:\", type(val_filenames), len(val_filenames))\n",
    "print (\"filenames:\", type(filenames), len(filenames))\n",
    "print (\"test_filenames:\", type(test_filenames), len(test_filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Vgg16().model\n",
    "#NB: utils.split_at returns two lists of layers split at the first matching layer type\n",
    "conv_layers,fc_layers = split_at(model, Convolution2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: 38\n",
      "conv_layers: 31\n",
      "fc_layers: 7\n"
     ]
    }
   ],
   "source": [
    "print (\"model:\", len(model.layers))\n",
    "print (\"conv_layers:\", len(conv_layers))\n",
    "print (\"fc_layers:\", len(fc_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 1000)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output_shape\n",
    "#(None, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lambda_4 (Lambda)                (None, 3, 224, 224)   0           lambda_input_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_40 (ZeroPadding2D) (None, 3, 226, 226)   0           lambda_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_40 (Convolution2D) (None, 64, 224, 224)  1792        zeropadding2d_40[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_41 (ZeroPadding2D) (None, 64, 226, 226)  0           convolution2d_40[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_41 (Convolution2D) (None, 64, 224, 224)  36928       zeropadding2d_41[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_16 (MaxPooling2D)   (None, 64, 112, 112)  0           convolution2d_41[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_42 (ZeroPadding2D) (None, 64, 114, 114)  0           maxpooling2d_16[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_42 (Convolution2D) (None, 128, 112, 112) 73856       zeropadding2d_42[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_43 (ZeroPadding2D) (None, 128, 114, 114) 0           convolution2d_42[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_43 (Convolution2D) (None, 128, 112, 112) 147584      zeropadding2d_43[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_17 (MaxPooling2D)   (None, 128, 56, 56)   0           convolution2d_43[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_44 (ZeroPadding2D) (None, 128, 58, 58)   0           maxpooling2d_17[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_44 (Convolution2D) (None, 256, 56, 56)   295168      zeropadding2d_44[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_45 (ZeroPadding2D) (None, 256, 58, 58)   0           convolution2d_44[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_45 (Convolution2D) (None, 256, 56, 56)   590080      zeropadding2d_45[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_46 (ZeroPadding2D) (None, 256, 58, 58)   0           convolution2d_45[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_46 (Convolution2D) (None, 256, 56, 56)   590080      zeropadding2d_46[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_18 (MaxPooling2D)   (None, 256, 28, 28)   0           convolution2d_46[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_47 (ZeroPadding2D) (None, 256, 30, 30)   0           maxpooling2d_18[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_47 (Convolution2D) (None, 512, 28, 28)   1180160     zeropadding2d_47[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_48 (ZeroPadding2D) (None, 512, 30, 30)   0           convolution2d_47[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_48 (Convolution2D) (None, 512, 28, 28)   2359808     zeropadding2d_48[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_49 (ZeroPadding2D) (None, 512, 30, 30)   0           convolution2d_48[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_49 (Convolution2D) (None, 512, 28, 28)   2359808     zeropadding2d_49[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_19 (MaxPooling2D)   (None, 512, 14, 14)   0           convolution2d_49[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_50 (ZeroPadding2D) (None, 512, 16, 16)   0           maxpooling2d_19[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_50 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_50[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_51 (ZeroPadding2D) (None, 512, 16, 16)   0           convolution2d_50[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_51 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_51[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_52 (ZeroPadding2D) (None, 512, 16, 16)   0           convolution2d_51[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_52 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_52[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_20 (MaxPooling2D)   (None, 512, 7, 7)     0           convolution2d_52[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 25088)         0           maxpooling2d_20[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 4096)          102764544   flatten_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (None, 4096)          0           dense_10[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_11 (Dense)                 (None, 4096)          16781312    dropout_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)              (None, 4096)          0           dense_11[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_12 (Dense)                 (None, 1000)          4097000     dropout_8[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "#NB64*64 = 4096\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model = Sequential(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "(512, 14, 14)\n"
     ]
    }
   ],
   "source": [
    "print (len(conv_model.layers))\n",
    "print (conv_model.output_shape[1:])\n",
    "#recall conv_model is model split at last Convolution2D layer\n",
    "#can see shape of last Convolution2D layer in the model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_features = conv_model.predict_generator(val_batches, val_batches.nb_sample)\n",
    "trn_features = conv_model.predict_generator(batches, batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_features: <type 'numpy.ndarray'> (136, 512, 14, 14)\n",
      "trn_features: <type 'numpy.ndarray'> (320, 512, 14, 14)\n"
     ]
    }
   ],
   "source": [
    "print (\"val_features:\", type(val_features), val_features.shape)\n",
    "print (\"trn_features:\", type(trn_features), trn_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 320 images belonging to 2 classes.\n",
      "Found 136 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#NB: utils.get_data(path, target_size=(224,224))\n",
    "#by default this uses array size 224 x 224. may want to look at other sizes for speed/accuracy optimisaiton\n",
    "trn = get_data(WORKING_TRAIN)\n",
    "val = get_data(WORKING_VALID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, we can precompute the output of all but the last dropout and dense layers, for creating the first stage of the model:\n",
    "model.pop()\n",
    "model.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: 36\n"
     ]
    }
   ],
   "source": [
    "print (\"model:\", len(model.layers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ll_val_feat: <type 'numpy.ndarray'> (136, 4096)\n",
      "ll_feat: <type 'numpy.ndarray'> (320, 4096)\n"
     ]
    }
   ],
   "source": [
    "ll_val_feat = model.predict_generator(val_batches, val_batches.nb_sample)\n",
    "ll_feat = model.predict_generator(batches, batches.nb_sample)\n",
    "print (\"ll_val_feat:\", type(ll_val_feat), ll_val_feat.shape)\n",
    "print (\"ll_feat:\", type(ll_feat), ll_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 306 images belonging to 1 classes.\n",
      "test: <type 'numpy.ndarray'> (306, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "test = get_data(WORKING_TEST)\n",
    "print (\"test:\", type(test), test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions automate creating a model that trains the last layer from scratch, and then adds those new layers on to the main model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ll_layers():\n",
    "    #creates and returns 3 layers\n",
    "    return [ \n",
    "        BatchNormalization(input_shape=(4096,)),\n",
    "        Dropout(0.5),\n",
    "        Dense(2, activation='softmax') \n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_last_layer(i):\n",
    "    ll_layers = get_ll_layers()\n",
    "    ll_model = Sequential(ll_layers)\n",
    "    ll_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    ll_model.optimizer.lr=1e-5\n",
    "    ll_model.fit(ll_feat, trn_labels, validation_data=(ll_val_feat, val_labels), nb_epoch=12)\n",
    "    ll_model.optimizer.lr=1e-7\n",
    "    ll_model.fit(ll_feat, trn_labels, validation_data=(ll_val_feat, val_labels), nb_epoch=1)\n",
    "    ll_model.save_weights(model_path+'ll_bn' + i + '.h5')\n",
    "\n",
    "    vgg = Vgg16()\n",
    "    model = vgg.model\n",
    "    model.pop(); model.pop(); model.pop()\n",
    "    for layer in model.layers: layer.trainable=False\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    ll_layers = get_ll_layers()\n",
    "    for layer in ll_layers: model.add(layer)\n",
    "    for l1,l2 in zip(ll_model.layers, model.layers[-3:]):\n",
    "        l2.set_weights(l1.get_weights())\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.save_weights(model_path+'bn' + i + '.h5')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_model(model):\n",
    "    #\n",
    "    layers = model.layers\n",
    "    last_conv_idx = [index for index,layer in enumerate(layers) \n",
    "                         if type(layer) is Convolution2D][-1]\n",
    "    #last_conv_idx = index position of last Convolution2D layer\n",
    "\n",
    "    conv_layers = layers[:last_conv_idx+1]\n",
    "    #all layers from start to last Convolution2D layer\n",
    "    conv_model = Sequential(conv_layers)\n",
    "    #conv_model = model composed of layers from start to last Convolution2D layer\n",
    "    #conv_model = model less the last 7 layers. ie:  MaxPooling2D  and Flatten, Dense, Dropout, Dense, Dropout, Dense layers.\n",
    "    fc_layers = layers[last_conv_idx+1:]\n",
    "    #fc_layers = layers after last Convolution2D layer\n",
    "    return conv_model, fc_layers, last_conv_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fc_layers(p, in_shape):\n",
    "    #creates new set of layers with dropout rate and shape specified\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=in_shape),\n",
    "        Flatten(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(4096, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(2, activation='softmax')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_dense_layers(i, model):\n",
    "    conv_model, fc_layers, last_conv_idx = get_conv_model(model)\n",
    "    conv_shape = conv_model.output_shape[1:]\n",
    "    fc_model = Sequential(get_fc_layers(0.5, conv_shape))\n",
    "    for l1,l2 in zip(fc_model.layers, fc_layers): \n",
    "        weights = l2.get_weights()\n",
    "        l1.set_weights(weights)\n",
    "    fc_model.compile(optimizer=Adam(1e-5), loss='categorical_crossentropy', \n",
    "                     metrics=['accuracy'])\n",
    "    fc_model.fit(trn_features, trn_labels, nb_epoch=2, \n",
    "         batch_size=batch_size, validation_data=(val_features, val_labels))\n",
    "\n",
    "    gen = image.ImageDataGenerator(rotation_range=10, width_shift_range=0.05, \n",
    "       width_zoom_range=0.05, zoom_range=0.05,\n",
    "       channel_shift_range=10, height_shift_range=0.05, shear_range=0.05, horizontal_flip=True)\n",
    "    batches = gen.flow(trn, trn_labels, batch_size=batch_size)\n",
    "    val_batches = image.ImageDataGenerator().flow(val, val_labels, \n",
    "                      shuffle=False, batch_size=batch_size)\n",
    "\n",
    "    for layer in conv_model.layers: layer.trainable = False\n",
    "    for layer in get_fc_layers(0.5, conv_shape): conv_model.add(layer)\n",
    "    for l1,l2 in zip(conv_model.layers[last_conv_idx+1:], fc_model.layers): \n",
    "        l1.set_weights(l2.get_weights())\n",
    "\n",
    "    conv_model.compile(optimizer=Adam(1e-5), loss='categorical_crossentropy', \n",
    "                       metrics=['accuracy'])\n",
    "    conv_model.save_weights(model_path+'no_dropout_bn' + i + '.h5')\n",
    "    conv_model.fit_generator(batches, samples_per_epoch=batches.N, nb_epoch=1, \n",
    "                            validation_data=val_batches, nb_val_samples=val_batches.N)\n",
    "    for layer in conv_model.layers[16:]: layer.trainable = True\n",
    "    conv_model.fit_generator(batches, samples_per_epoch=batches.N, nb_epoch=8, \n",
    "                            validation_data=val_batches, nb_val_samples=val_batches.N)\n",
    "\n",
    "    conv_model.optimizer.lr = 1e-7\n",
    "    conv_model.fit_generator(batches, samples_per_epoch=batches.N, nb_epoch=10, \n",
    "                            validation_data=val_batches, nb_val_samples=val_batches.N)\n",
    "    conv_model.save_weights(model_path + 'aug' + i + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def train_dense_layers(i, model):\n",
    "    #\n",
    "    conv_model, fc_layers, last_conv_idx = get_conv_model(model)\n",
    "    #\n",
    "    \n",
    "    conv_shape = conv_model.output_shape[1:]\n",
    "    #conv_shape = shape of last Convolution2D layer\n",
    "    fc_model = Sequential(get_fc_layers(0.5, conv_shape))\n",
    "    #creates new set of layers with dropout rate and shape specified\n",
    "    \n",
    "    #fc_layers = list layers after last Convolution2D layer in model\n",
    "    for l1,l2 in zip(fc_model.layers, fc_layers): \n",
    "        #copies weights from layers in original model to layers in new model\n",
    "        weights = l2.get_weights()\n",
    "        l1.set_weights(weights)\n",
    "    \n",
    "    fc_model.compile(optimizer=Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    #compiling model required before can fit.\n",
    "    \n",
    "    fc_model.fit(trn_features, trn_labels, nb_epoch=2, batch_size=batch_size, validation_data=(val_features, val_labels))\n",
    "    #only two epoch here. maybe increase in future and look @ history.\n",
    "\n",
    "    #now do the augmentation\n",
    "    gen = image.ImageDataGenerator(rotation_range=10, \n",
    "                                   width_shift_range=0.05, \n",
    "                                   width_zoom_range=0.05, \n",
    "                                   zoom_range=0.05, \n",
    "                                   channel_shift_range=10, \n",
    "                                   height_shift_range=0.05, \n",
    "                                   shear_range=0.05, \n",
    "                                   horizontal_flip=True)\n",
    "    \n",
    "    batches = gen.flow(trn, trn_labels, batch_size=batch_size)\n",
    "    val_batches = image.ImageDataGenerator().flow(val, val_labels, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "    #set all layers in conv_model to non trainable\n",
    "    for layer in conv_model.layers: \n",
    "        layer.trainable = False\n",
    "    \n",
    "    #set dropout rate and shape for new layers to be added to conv_model\n",
    "    for layer in get_fc_layers(0.5, conv_shape): \n",
    "        conv_model.add(layer)\n",
    "        \n",
    "    #copy weights from layers in fc_model to conv_model.\n",
    "    for l1,l2 in zip(conv_model.layers[last_conv_idx+1:], fc_model.layers): \n",
    "        l1.set_weights(l2.get_weights())\n",
    "\n",
    "    #compile model required before it can be used\n",
    "    conv_model.compile(optimizer=Adam(1e-5), loss='categorical_crossentropy', \n",
    "                       metrics=['accuracy'])\n",
    "    \n",
    "    #\n",
    "    conv_model.save_weights(model_path+'no_dropout_bn' + i + '.h5')\n",
    "    \n",
    "    #nb: at this stage, layers in conv_model are untrainable\n",
    "    conv_model.fit_generator(batches, samples_per_epoch=batches.N, nb_epoch=1, \n",
    "                            validation_data=val_batches, nb_val_samples=val_batches.N)\n",
    "    \n",
    "    #nb: previously made these layers untrainable, now make them trainable\n",
    "    for layer in conv_model.layers[16:]: \n",
    "        layer.trainable = True\n",
    "    \n",
    "    #do some fitting, multiple epochs.\n",
    "    conv_model.fit_generator(batches, samples_per_epoch=batches.N, nb_epoch=8, \n",
    "                            validation_data=val_batches, nb_val_samples=val_batches.N)\n",
    "\n",
    "    conv_model.optimizer.lr = 1e-7\n",
    "    #set lower training lr value, do more epochs\n",
    "    conv_model.fit_generator(batches, samples_per_epoch=batches.N, nb_epoch=10, \n",
    "                            validation_data=val_batches, nb_val_samples=val_batches.N)\n",
    "    \n",
    "    #\n",
    "    conv_model.save_weights(model_path + 'aug' + i + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 320 samples, validate on 136 samples\n",
      "Epoch 1/12\n",
      "320/320 [==============================] - 0s - loss: 1.0892 - acc: 0.6031 - val_loss: 0.9481 - val_acc: 0.6029\n",
      "Epoch 2/12\n",
      "320/320 [==============================] - 0s - loss: 1.0221 - acc: 0.6031 - val_loss: 0.9318 - val_acc: 0.6029\n",
      "Epoch 3/12\n",
      "320/320 [==============================] - 0s - loss: 1.0349 - acc: 0.5750 - val_loss: 0.9172 - val_acc: 0.6029\n",
      "Epoch 4/12\n",
      "320/320 [==============================] - 0s - loss: 0.9606 - acc: 0.6219 - val_loss: 0.9051 - val_acc: 0.6103\n",
      "Epoch 5/12\n",
      "320/320 [==============================] - 0s - loss: 0.9864 - acc: 0.5656 - val_loss: 0.8959 - val_acc: 0.6103\n",
      "Epoch 6/12\n",
      "320/320 [==============================] - 0s - loss: 1.0501 - acc: 0.5406 - val_loss: 0.8898 - val_acc: 0.6250\n",
      "Epoch 7/12\n",
      "320/320 [==============================] - 0s - loss: 0.9569 - acc: 0.5906 - val_loss: 0.8834 - val_acc: 0.6324\n",
      "Epoch 8/12\n",
      "320/320 [==============================] - 0s - loss: 0.9545 - acc: 0.6188 - val_loss: 0.8774 - val_acc: 0.6250\n",
      "Epoch 9/12\n",
      "320/320 [==============================] - 0s - loss: 0.9143 - acc: 0.6344 - val_loss: 0.8719 - val_acc: 0.6250\n",
      "Epoch 10/12\n",
      "320/320 [==============================] - 0s - loss: 0.9910 - acc: 0.5844 - val_loss: 0.8667 - val_acc: 0.6471\n",
      "Epoch 11/12\n",
      "320/320 [==============================] - 0s - loss: 0.8925 - acc: 0.6250 - val_loss: 0.8623 - val_acc: 0.6471\n",
      "Epoch 12/12\n",
      "320/320 [==============================] - 0s - loss: 0.9172 - acc: 0.6125 - val_loss: 0.8589 - val_acc: 0.6471\n",
      "Train on 320 samples, validate on 136 samples\n",
      "Epoch 1/1\n",
      "320/320 [==============================] - 0s - loss: 0.9318 - acc: 0.6281 - val_loss: 0.8560 - val_acc: 0.6471\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You called `set_weights(weights)` on layer \"batchnormalization_7\" with a  weight list of length 0, but the layer was expecting 4 weights. Provided weights: []...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-6d8fda4e55b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_last_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_dense_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-95-1e726a21420a>\u001b[0m in \u001b[0;36mtrain_dense_layers\u001b[0;34m(i, model)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0ml1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     fc_model.compile(optimizer=Adam(1e-5), loss='categorical_crossentropy', \n\u001b[1;32m     10\u001b[0m                      metrics=['accuracy'])\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m    973\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m                              \u001b[0;34m' weights. Provided weights: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m                              str(weights)[:50] + '...')\n\u001b[0m\u001b[1;32m    976\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You called `set_weights(weights)` on layer \"batchnormalization_7\" with a  weight list of length 0, but the layer was expecting 4 weights. Provided weights: []..."
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    i = str(i)\n",
    "    model = train_last_layer(i)\n",
    "    train_dense_layers(i, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
