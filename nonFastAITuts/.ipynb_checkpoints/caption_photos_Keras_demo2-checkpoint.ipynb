{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5110)\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOMEPATH = \"/home/ubuntu/fastai/fastai_bmt_reworking/nonFastAITuts/\"\n",
    "DATAPATH = \"/home/ubuntu/fastai/data/Flicker8k_Dataset/\"\n",
    "PICKLEPATH = \"/home/ubuntu/fastai/data/\"\n",
    "FLICKTEXTPATH = \"/home/ubuntu/fastai/data/Flicker8k_Text/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(filename):\n",
    "\tdoc = load_doc(filename)\n",
    "\tdescriptions = dict()\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t# split line by white space\n",
    "\t\ttokens = line.split()\n",
    "\t\t# split id from description\n",
    "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
    "\t\t# store\n",
    "\t\tdescriptions[image_id] = ' '.join(image_desc)\n",
    "\treturn descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a tokenizer given caption descriptions\n",
    "def create_tokenizer(descriptions):\n",
    "\tlines = list(descriptions.values())\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a single photo intended as input for the VGG feature extractor model\n",
    "def load_photo(filename):\n",
    "\timage = load_img(filename, target_size=(224, 224))\n",
    "\t# convert the image pixels to a numpy array\n",
    "\timage = img_to_array(image)\n",
    "\t# reshape data for the model\n",
    "\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "\t# prepare the image for the VGG model\n",
    "\timage = preprocess_input(image)[0]\n",
    "\t# get image id\n",
    "\timage_id = filename.split('/')[-1].split('.')[0]\n",
    "\treturn image, image_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_length, desc, image):\n",
    "\tXimages, XSeq, y = list(), list(),list()\n",
    "\tvocab_size = len(tokenizer.word_index) + 1\n",
    "\t# integer encode the description\n",
    "\tseq = tokenizer.texts_to_sequences([desc])[0]\n",
    "\t# split one sequence into multiple X,y pairs\n",
    "\tfor i in range(1, len(seq)):\n",
    "\t\t# select\n",
    "\t\tin_seq, out_seq = seq[:i], seq[i]\n",
    "\t\t# pad input sequence\n",
    "\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "\t\t# encode output sequence\n",
    "\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\t\t# store\n",
    "\t\tXimages.append(image)\n",
    "\t\tXSeq.append(in_seq)\n",
    "\t\ty.append(out_seq)\n",
    "\tXimages, XSeq, y = array(Ximages), array(XSeq), array(y)\n",
    "\treturn [Ximages, XSeq, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator, intended to be used in a call to model.fit_generator()\n",
    "def data_generator(descriptions, tokenizer, max_length):\n",
    "\t# loop for ever over images\n",
    "\twhile 1:\n",
    "\t\tfor name in listdir(DATAPATH):\n",
    "\t\t\t# load an image from file\n",
    "\t\t\tfilename = DATAPATH + name\n",
    "\t\t\timage, image_id = load_photo(filename)\n",
    "\t\t\t# create word sequences\n",
    "\t\t\tdesc = descriptions[image_id]\n",
    "\t\t\tin_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc, image)\n",
    "\t\t\tyield [[in_img, in_seq], out_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description Length: 28\n",
      "(7, 224, 224, 3)\n",
      "(7, 28)\n",
      "(7, 4485)\n"
     ]
    }
   ],
   "source": [
    "# load mapping of ids to descriptions\n",
    "descriptions = load_clean_descriptions(FLICKTEXTPATH+'descriptions.txt')\n",
    "# integer encode sequences of words\n",
    "tokenizer = create_tokenizer(descriptions)\n",
    "# pad to fixed length\n",
    "max_length = max(len(s.split()) for s in list(descriptions.values()))\n",
    "print('Description Length: %d' % max_length)\n",
    " \n",
    "# test the data generator\n",
    "generator = data_generator(descriptions, tokenizer, max_length)\n",
    "inputs, outputs = next(generator)\n",
    "print(inputs[0].shape)\n",
    "print(inputs[1].shape)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
