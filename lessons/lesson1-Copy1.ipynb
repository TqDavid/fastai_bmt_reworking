{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the first week of the first deep learning certificate! We're going to use convolutional neural networks (CNNs) to allow our computer to see - something that is only possible thanks to deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to this week's task: 'Dogs vs Cats'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to try to create a model to enter the [Dogs vs Cats](https://www.kaggle.com/c/dogs-vs-cats) competition at Kaggle. There are 25,000 labelled dog and cat photos available for training, and 12,500 in the test set that we have to try to label for this competition. According to the Kaggle web-site, when this competition was launched (end of 2013): *\"**State of the art**: The current literature suggests machine classifiers can score above 80% accuracy on this task\"*. So if we can beat 80%, then we will be at the cutting edge as of 2013!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn't too much to do to get started - just a few simple configuration steps.\n",
    "\n",
    "This shows plots in the web page itself - we always wants to use this when using jupyter notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sys.version_info:', sys.version_info(major=2, minor=7, micro=12, releaselevel='final', serial=0))\n",
      "alabaster==0.7.9\n",
      "anaconda-clean==1.0\n",
      "anaconda-client==1.5.1\n",
      "anaconda-navigator==1.3.1\n",
      "argcomplete==1.0.0\n",
      "astroid==1.4.7\n",
      "astropy==1.2.1\n",
      "babel==2.3.4\n",
      "backports-abc==0.4\n",
      "backports.shutil-get-terminal-size==1.0.0\n",
      "backports.ssl-match-hostname==3.4.0.2\n",
      "bcolz==1.0.0\n",
      "beautifulsoup4==4.5.1\n",
      "bitarray==0.8.1\n",
      "blaze==0.10.1\n",
      "bokeh==0.12.2\n",
      "boto==2.42.0\n",
      "bottleneck==1.1.0\n",
      "cdecimal==2.3\n",
      "cffi==1.7.0\n",
      "chest==0.2.3\n",
      "click==6.6\n",
      "cloudpickle==0.2.1\n",
      "clyent==1.2.2\n",
      "colorama==0.3.7\n",
      "conda-build==2.0.6\n",
      "conda==4.2.9\n",
      "configobj==5.0.6\n",
      "configparser==3.5.0\n",
      "contextlib2==0.5.3\n",
      "cryptography==1.5\n",
      "cycler==0.10.0\n",
      "cython==0.24.1\n",
      "cytoolz==0.8.0\n",
      "dask==0.11.0\n",
      "datashape==0.5.2\n",
      "decorator==4.0.10\n",
      "dill==0.2.5\n",
      "docutils==0.12\n",
      "dynd==0.7.3.dev1\n",
      "enum34==1.1.6\n",
      "et-xmlfile==1.0.1\n",
      "fastcache==1.0.2\n",
      "filelock==2.0.6\n",
      "flask-cors==2.1.2\n",
      "flask==0.11.1\n",
      "funcsigs==1.0.2\n",
      "functools32==3.2.3.post2\n",
      "futures==3.0.5\n",
      "gevent==1.1.2\n",
      "greenlet==0.4.10\n",
      "grin==1.2.1\n",
      "h5py==2.6.0\n",
      "heapdict==1.0.0\n",
      "idna==2.1\n",
      "imagesize==0.7.1\n",
      "ipaddress==1.0.16\n",
      "ipykernel==4.5.0\n",
      "ipython-genutils==0.1.0\n",
      "ipython==5.1.0\n",
      "ipywidgets==5.2.2\n",
      "itsdangerous==0.24\n",
      "jdcal==1.2\n",
      "jedi==0.9.0\n",
      "jinja2==2.8\n",
      "jsonschema==2.5.1\n",
      "jupyter-client==4.4.0\n",
      "jupyter-console==5.0.0\n",
      "jupyter-core==4.2.0\n",
      "jupyter==1.0.0\n",
      "keras==1.1.0\n",
      "lazy-object-proxy==1.2.1\n",
      "llvmlite==0.13.0\n",
      "locket==0.2.0\n",
      "lxml==3.6.4\n",
      "markupsafe==0.23\n",
      "matplotlib==1.5.3\n",
      "mistune==0.7.3\n",
      "mpmath==0.19\n",
      "multipledispatch==0.4.8\n",
      "nb-anacondacloud==1.2.0\n",
      "nb-conda-kernels==2.0.0\n",
      "nb-conda==2.0.0\n",
      "nbconvert==4.2.0\n",
      "nbformat==4.1.0\n",
      "nbpresent==3.0.2\n",
      "networkx==1.11\n",
      "nltk==3.2.1\n",
      "nose==1.3.7\n",
      "notebook==4.2.3\n",
      "numba==0.28.1+0.gfe99fbc.dirty\n",
      "numexpr==2.6.1\n",
      "numpy==1.11.1\n",
      "odo==0.5.0\n",
      "openpyxl==2.3.2\n",
      "pandas==0.18.1\n",
      "partd==0.3.6\n",
      "path.py==0.0.0\n",
      "pathlib2==2.1.0\n",
      "patsy==0.4.1\n",
      "pep8==1.7.0\n",
      "pexpect==4.0.1\n",
      "pickleshare==0.7.4\n",
      "pillow==3.3.1\n",
      "pip==9.0.1\n",
      "pkginfo==1.3.2\n",
      "ply==3.9\n",
      "prompt-toolkit==1.0.3\n",
      "psutil==4.3.1\n",
      "ptyprocess==0.5.1\n",
      "py==1.4.31\n",
      "pyasn1==0.1.9\n",
      "pycairo==1.10.0\n",
      "pycosat==0.6.1\n",
      "pycparser==2.14\n",
      "pycrypto==2.6.1\n",
      "pycurl==7.43.0\n",
      "pyflakes==1.3.0\n",
      "pygments==2.1.3\n",
      "pylint==1.5.4\n",
      "pyopenssl==16.0.0\n",
      "pyparsing==2.1.4\n",
      "pytest==2.9.2\n",
      "python-dateutil==2.5.3\n",
      "pytz==2016.6.1\n",
      "pyyaml==3.12\n",
      "pyzmq==15.4.0\n",
      "qtawesome==0.3.3\n",
      "qtconsole==4.2.1\n",
      "qtpy==1.1.2\n",
      "redis==2.10.5\n",
      "requests==2.11.1\n",
      "rope==0.9.4\n",
      "ruamel-yaml==-VERSION\n",
      "scikit-image==0.12.3\n",
      "scikit-learn==0.17.1\n",
      "scipy==0.18.1\n",
      "setuptools==27.2.0\n",
      "simplegeneric==0.8.1\n",
      "singledispatch==3.4.0.3\n",
      "six==1.10.0\n",
      "snowballstemmer==1.2.1\n",
      "sockjs-tornado==1.0.3\n",
      "sphinx==1.4.6\n",
      "spyder==3.0.0\n",
      "sqlalchemy==1.0.13\n",
      "statsmodels==0.6.1\n",
      "sympy==1.0\n",
      "tables==3.2.3.1\n",
      "terminado==0.6\n",
      "theano==0.8.2\n",
      "toolz==0.8.0\n",
      "tornado==4.4.1\n",
      "traitlets==4.3.0\n",
      "unicodecsv==0.14.1\n",
      "wcwidth==0.1.7\n",
      "werkzeug==0.11.11\n",
      "wheel==0.29.0\n",
      "widgetsnbextension==1.2.6\n",
      "wrapt==1.10.6\n",
      "xlrd==1.0.0\n",
      "xlsxwriter==0.9.3\n",
      "xlwt==1.1.2\n"
     ]
    }
   ],
   "source": [
    "#added a few utility methods\n",
    "import os\n",
    "import sys\n",
    "print (\"sys.version_info:\", sys.version_info)\n",
    "import time\n",
    "import pip\n",
    "installed_packages = pip.get_installed_distributions()\n",
    "installed_packages_list = sorted([\"%s==%s\" % (i.key, i.version)\n",
    "     for i in installed_packages])\n",
    "#print(installed_packages_list)\n",
    "for package in installed_packages_list:\n",
    "    print (package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "print (\"loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define path to data: (It's a good idea to put it in a subdirectory of your notebooks folder, and then exclude that directory from git control by adding it to .gitignore.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dogs', 'cats']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"/home/ubuntu/data2/dogscats/valid\")\n",
    "#nb: this just reflects where jupyter notebooks was run from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('path:', '/home/ubuntu/data2/dogscats/')\n",
      "('/home/ubuntu/data2/dogscats/sample/train/cats/', ', #:', 80)\n",
      "('/home/ubuntu/data2/dogscats/sample/train/dogs/', ', #:', 80)\n",
      "('/home/ubuntu/data2/dogscats/train/cats/', ', #:', 11500)\n",
      "('/home/ubuntu/data2/dogscats/train/dogs/', ', #:', 11500)\n",
      "('/home/ubuntu/data2/dogscats/valid/dogs/', ', #:', 1000)\n",
      "('/home/ubuntu/data2/dogscats/valid/dogs/', ', #:', 1000)\n"
     ]
    }
   ],
   "source": [
    "# path = \"data/dogscats/\"\n",
    "#be sure to load correct data for this course. the kaggle dataset only has split train and test.\n",
    "#this model uses train / verify / test]\n",
    "#wget http://files.fast.ai/files/dogscats.zip to \"/home/ubuntu/data2/\" then unzip\n",
    "path = \"/home/ubuntu/data2/dogscats/\"\n",
    "print (\"path:\", path)\n",
    "#count files in train/verify/test directories\n",
    "dirList = ['sample/train/cats/', 'sample/train/dogs/', \n",
    "           'train/cats/', 'train/dogs/', \n",
    "           'valid/dogs/', 'valid/dogs/']\n",
    "for dir_ in dirList:\n",
    "    pathDir = path + dir_\n",
    "    #files = [f for f in os.listdir(pathDir) if os.path.isfile(f)]\n",
    "    #print(\"# of files in directory\",pathDir,\" = \", files)\n",
    "    numFiles = len([name for name in os.listdir(pathDir) if os.path.isfile(os.path.join(pathDir, name))])\n",
    "    print (pathDir, \", #:\", numFiles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few basic libraries that we'll need for the initial exercises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division,print_function\n",
    "\n",
    "import json\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, linewidth=100)\n",
    "from matplotlib import pyplot as plt\n",
    "print (\"loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a file most imaginatively called 'utils.py' to store any little convenience functions we'll want to use. We will discuss these as we use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "#set dir to enable loading.\n",
    "os.chdir(\"/home/ubuntu/\")\n",
    "import utils; reload(utils)\n",
    "from utils import plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use a pretrained VGG model with our **Vgg16** class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step is simply to use a model that has been fully created for us, which can recognise a wide variety (1,000 categories) of images. We will use 'VGG', which won the 2014 Imagenet competition, and is a very simple model to create and understand. The VGG Imagenet team created both a larger, slower, slightly more accurate model (*VGG  19*) and a smaller, faster model (*VGG 16*). We will be using VGG 16 since the much slower performance of VGG19 is generally not worth the very minor improvement in accuracy.\n",
    "\n",
    "We have created a python class, *Vgg16*, which makes using the VGG 16 model very straightforward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The punchline: state of the art custom model in 7 lines of code\n",
    "\n",
    "Here's everything you need to do to get >97% accuracy on the Dogs vs Cats dataset - we won't analyze how it works behind the scenes yet, since at this stage we're just going to focus on the minimum necessary to actually do useful work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As large as you can, but no larger than 64 is recommended. \n",
    "# If you have an older or cheaper GPU, you'll run out of memory, so will have to decrease this.\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom libraries loaded\n"
     ]
    }
   ],
   "source": [
    "# Import our class, and instantiate\n",
    "import vgg16; reload(vgg16)\n",
    "from vgg16 import Vgg16\n",
    "print (\"custom libraries loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "getting training data\n",
      "Found 23000 images belonging to 2 classes.\n",
      "getting validation data\n",
      "Found 2000 images belonging to 2 classes.\n",
      "vgg.finetune training data\n",
      "fitting validation data\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 721s - loss: 0.1217 - acc: 0.9673 - val_loss: 0.0379 - val_acc: 0.9880\n",
      "end\n",
      "--- 732.646816015 seconds to run this block ---\n"
     ]
    }
   ],
   "source": [
    "print (\"starting\")\n",
    "start_time = time.time()\n",
    "vgg = Vgg16()\n",
    "# Grab a few images at a time for training and validation.\n",
    "# NB: They must be in subdirectories named based on their category\n",
    "#takes approx 752 seconds to run on aws p2 instance.\n",
    "print(\"getting training data\")\n",
    "#nb: refer above filecounts 11500+11500 = 23000\n",
    "batches = vgg.get_batches(path+'train', batch_size=batch_size)\n",
    "print(\"getting validation data\")\n",
    "#nb: refer above filecounts 1000+1000 = 2000\n",
    "val_batches = vgg.get_batches(path+'valid', batch_size=batch_size*2)\n",
    "print(\"vgg.finetune training data\")\n",
    "vgg.finetune(batches)\n",
    "print (\"fitting validation data\")\n",
    "vgg.fit(batches, val_batches, nb_epoch=1)\n",
    "print (\"end\")\n",
    "print(\"--- %s seconds to run this block ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'instance'>\n",
      "<keras.models.Sequential object at 0x7f13ee03ed50>\n",
      "current dir: /home/ubuntu/data2/dogscats\n",
      "/home/ubuntu/data2/dogscats/\n"
     ]
    }
   ],
   "source": [
    "print (type(vgg))\n",
    "print(vgg.model)#keras.models.Sequential object\n",
    "#set cwd\n",
    "os.chdir(path)\n",
    "print (\"current dir:\", os.getcwd())\n",
    "vgg.model.save_weights('results_firstFit.h5')\n",
    "print (path)\n",
    "\n",
    "#xxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "batch_size: 64\n",
      "path: /home/ubuntu/data2/dogscats/\n",
      "Found 2000 images belonging to 2 classes.\n",
      "end\n",
      "--- 62.5995061398 seconds to run this block ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\npath+'valid'\\nfind . -type f | wc -l\\n2000 files in path+'valid'  cats and dogs, 1000 files each dir\\n\\npath+'test'\\nfound zero files in categories - because test is not categorized. all files in one dir.\\n\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"starting\")\n",
    "print (\"batch_size:\", batch_size)\n",
    "print (\"path:\", path)\n",
    "\n",
    "start_time = time.time()\n",
    "batches1, preds1 = vgg.test(path+'valid', batch_size*2)\n",
    "print (\"end\")\n",
    "print(\"--- %s seconds to run this block ---\" % (time.time() - start_time))\n",
    "\n",
    "'''\n",
    "path+'valid'\n",
    "find . -type f | wc -l\n",
    "2000 files in path+'valid'  cats and dogs, 1000 files each dir\n",
    "\n",
    "path+'test'\n",
    "found zero files in categories - because test is not categorized. all files in one dir.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from utils import plots, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-67b7344a12d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "print (type(labels))\n",
    "print (type(preds1))\n",
    "cm = confusion_matrix(labels, preds1)\n",
    "plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above will work for any image recognition task, with any number of categories! All you have to do is to put your images into one folder per category, and run the code above.\n",
    "\n",
    "Let's take a look at how this works, step by step..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Vgg16 for basic image recognition\n",
    "\n",
    "Let's start off by using the *Vgg16* class to recognise the main imagenet category for each image.\n",
    "\n",
    "We won't be able to enter the Cats vs Dogs competition with an Imagenet model alone, since 'cat' and 'dog' are not categories in Imagenet - instead each individual breed is a separate category. However, we can use it to see how well it can recognise the images, which is a good first step.\n",
    "\n",
    "First, create a Vgg16 object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg = Vgg16()\n",
    "print (type(vgg))\n",
    "print (\"list of available methods :\", dir(vgg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vgg16 is built on top of *Keras* (which we will be learning much more about shortly!), a flexible, easy to use deep learning library that sits on top of Theano or Tensorflow. Keras reads groups of images and labels in *batches*, using a fixed directory structure, where images from each category for training must be placed in a separate folder.\n",
    "\n",
    "Let's grab batches of data from our training folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "batches = vgg.get_batches(path+'train', batch_size=4)\n",
    "print(\"--- %s seconds to run this block ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(BTW, when Keras refers to 'classes', it doesn't mean python classes - but rather it refers to the categories of the labels, such as 'pug', or 'tabby'.)\n",
    "\n",
    "*Batches* is just a regular python iterator. Each iteration returns both the images themselves, as well as the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "imgs,labels = next(batches)\n",
    "print(\"--- %s seconds to run this block ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the labels for each image are an array, containing a 1 in the first position if it's a cat, and in the second position if it's a dog. This approach to encoding categorical variables, where an array containing just a single 1 in the position corresponding to the category, is very common in deep learning. It is called *one hot encoding*. \n",
    "\n",
    "The arrays contain two elements, because we have two categories (cat, and dog). If we had three categories (e.g. cats, dogs, and kangaroos), then the arrays would each contain two 0's, and one 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "plots(imgs, titles=labels)\n",
    "print(\"--- %s seconds to run this block ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now pass the images to Vgg16's predict() function to get back probabilities, category indexes, and category names for each image's VGG prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print (vgg.predict(imgs, True))\n",
    "print(\"--- %s seconds to run this block ---\" % (time.time() - start_time))\n",
    "\n",
    "vgg.predict(imgs, True)\n",
    "#nb: video example is providing predictions to 4 decimal places cf 1 or 0\n",
    "#nbb: need to check where/how/why the predictions are being truncated to integers.\n",
    "#nbb: video example is showing more specific categories than 'dogs' / 'cats'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The category indexes are based on the ordering of categories used in the VGG model - e.g here are the first four:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(vgg.classes[:4])\n",
    "#these classes are not very useful.\n",
    "for class_ in vgg.classes[:4]:\n",
    "    print (class_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note that, other than creating the Vgg16 object, none of these steps are necessary to build a model; they are just showing how to use the class to view imagenet predictions.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use our Vgg16 class to finetune a Dogs vs Cats model\n",
    "\n",
    "To change our model so that it outputs \"cat\" vs \"dog\", instead of one of 1,000 very specific categories, we need to use a process called \"finetuning\". Finetuning looks from the outside to be identical to normal machine learning training - we provide a training set with data and labels to learn from, and a validation set to test against. The model learns a set of parameters based on the data provided.\n",
    "\n",
    "However, the difference is that we start with a model that is already trained to solve a similar problem. The idea is that many of the parameters should be very similar, or the same, between the existing model, and the model we wish to create. Therefore, we only select a subset of parameters to train, and leave the rest untouched. This happens automatically when we call *fit()* after calling *finetune()*.\n",
    "\n",
    "We create our batches just like before, and making the validation set available as well. A 'batch' (or *mini-batch* as it is commonly known) is simply a subset of the training data - we use a subset at a time when training or predicting, in order to speed up training, and to avoid running out of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "batches = vgg.get_batches(path+'train', batch_size=batch_size)\n",
    "val_batches = vgg.get_batches(path+'valid', batch_size=batch_size)\n",
    "print(\"--- %s seconds to run this block ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling *finetune()* modifies the model such that it will be trained based on the data in the batches provided - in this case, to predict either 'dog' or 'cat'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "vgg.finetune(batches)\n",
    "print(\"--- %s seconds to run this block ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we *fit()* the parameters of the model using the training data, reporting the accuracy on the validation set after every epoch. (An *epoch* is one full pass through the training data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#takes long time to run 12-15 minutes\n",
    "start_time = time.time()\n",
    "vgg.fit(batches, val_batches, nb_epoch=1)\n",
    "print(\"--- %s seconds to run this block ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That shows all of the steps involved in using the Vgg16 class to create an image recognition model using whatever labels you are interested in. For instance, this process could classify paintings by style, or leaves by type of disease, or satellite photos by type of crop, and so forth.\n",
    "\n",
    "Next up, we'll dig one level deeper to see what's going on in the Vgg16 class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nb: code above achieves ~96% accuracy.\n",
    "start_time = time.time()\n",
    "vgg.fit(batches, val_batches, nb_epoch=1)\n",
    "print(\"--- %s seconds to run this block ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a VGG model from scratch in Keras\n",
    "\n",
    "For the rest of this tutorial, we will not be using the Vgg16 class at all. Instead, we will recreate from scratch the functionality we just used. This is not necessary if all you want to do is use the existing model - but if you want to create your own models, you'll need to understand these details. It will also help you in the future when you debug any problems with your models, since you'll understand what's going on behind the scenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup\n",
    "\n",
    "We need to import all the modules we'll be using from numpy, scipy, and keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy.random import random, permutation\n",
    "from scipy import misc, ndimage\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers import Input\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "\n",
    "from keras.preprocessing import image\n",
    "print (\"libraries loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the mappings from VGG ids to imagenet category ids and descriptions, for display purposes later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "FILES_PATH = 'http://files.fast.ai/models/'; CLASS_FILE='imagenet_class_index.json'\n",
    "# Keras' get_file() is a handy function that downloads files, and caches them for re-use later\n",
    "fpath = get_file(CLASS_FILE, FILES_PATH+CLASS_FILE, cache_subdir='models')\n",
    "with open(fpath) as f: class_dict = json.load(f)\n",
    "# Convert dictionary with string indexes into an array\n",
    "classes = [class_dict[str(i)][1] for i in range(len(class_dict))]\n",
    "print(\"--- %s seconds to run this block ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a few examples of the categories we just imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classes[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation\n",
    "\n",
    "Creating the model involves creating the model architecture, and then loading the model weights into that architecture. We will start by defining the basic pieces of the VGG architecture.\n",
    "\n",
    "VGG has just one type of convolutional block, and one type of fully connected ('dense') block. Here's the convolutional block definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ConvBlock(layers, model, filters):\n",
    "    for i in range(layers): \n",
    "        model.add(ZeroPadding2D((1,1)))\n",
    "        model.add(Convolution2D(filters, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "print(\"method loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and here's the fully-connected definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def FCBlock(model):\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "print(\"method loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the VGG model was trained in 2014, the creators subtracted the average of each of the three (R,G,B) channels first, so that the data for each channel had a mean of zero. Furthermore, their software that expected the channels to be in B,G,R order, whereas Python by default uses R,G,B. We need to preprocess our data to make these two changes, so that it is compatible with the VGG model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Mean of each channel as provided by VGG researchers\n",
    "vgg_mean = np.array([123.68, 116.779, 103.939]).reshape((3,1,1))\n",
    "\n",
    "def vgg_preprocess(x):\n",
    "    x = x - vgg_mean     # subtract mean\n",
    "    return x[:, ::-1]    # reverse axis bgr->rgb\n",
    "print(\"method loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to define the VGG model architecture - look at how simple it is, now that we have the basic blocks defined!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def VGG_16():\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(vgg_preprocess, input_shape=(3,224,224)))\n",
    "\n",
    "    ConvBlock(2, model, 64)\n",
    "    ConvBlock(2, model, 128)\n",
    "    ConvBlock(3, model, 256)\n",
    "    ConvBlock(3, model, 512)\n",
    "    ConvBlock(3, model, 512)\n",
    "\n",
    "    model.add(Flatten())\n",
    "    FCBlock(model)\n",
    "    FCBlock(model)\n",
    "    model.add(Dense(1000, activation='softmax'))\n",
    "    return model\n",
    "print(\"method loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll learn about what these different blocks do later in the course. For now, it's enough to know that:\n",
    "\n",
    "- Convolution layers are for finding patterns in images\n",
    "- Dense (fully connected) layers are for combining patterns across an image\n",
    "\n",
    "Now that we've defined the architecture, we can create the model like any python object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = VGG_16()\n",
    "print(\"type(model):\", type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as the architecture, we need the weights that the VGG creators trained. The weights are the part of the model that is learnt from the data, whereas the architecture is pre-defined based on the nature of the problem. \n",
    "\n",
    "Downloading pre-trained weights is much preferred to training the model ourselves, since otherwise we would have to download the entire Imagenet archive, and train the model for many days! It's very helpful when researchers release their weights, as they did here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fpath = get_file('vgg16.h5', FILES_PATH+'vgg16.h5', cache_subdir='models')\n",
    "model.load_weights(fpath)\n",
    "print(\"weights loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting imagenet predictions\n",
    "\n",
    "The setup of the imagenet model is now complete, so all we have to do is grab a batch of images and call *predict()* on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides functionality to create batches of data from directories containing images; all we have to do is to define the size to resize the images to, what type of labels to create, whether to randomly shuffle the images, and how many images to include in each batch. We use this little wrapper to define some helpful defaults appropriate for imagenet data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_batches(dirname, gen=image.ImageDataGenerator(), shuffle=True, \n",
    "                batch_size=batch_size, class_mode='categorical'):\n",
    "    return gen.flow_from_directory(path+dirname, target_size=(224,224), \n",
    "                class_mode=class_mode, shuffle=shuffle, batch_size=batch_size)\n",
    "print(\"method loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we can use exactly the same steps as before to look at predictions from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batches = get_batches('train', batch_size=batch_size)\n",
    "val_batches = get_batches('valid', batch_size=batch_size)\n",
    "imgs,labels = next(batches)\n",
    "\n",
    "# This shows the 'ground truth'\n",
    "plots(imgs, titles=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VGG model returns 1,000 probabilities for each image, representing the probability that the model assigns to each possible imagenet category for each image. By finding the index with the largest probability (with *np.argmax()*) we can find the predicted label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pred_batch(imgs):\n",
    "    preds = model.predict(imgs)\n",
    "    idxs = np.argmax(preds, axis=1)\n",
    "\n",
    "    print('Shape: {}'.format(preds.shape))\n",
    "    print('First 5 classes: {}'.format(classes[:5]))\n",
    "    print('First 5 probabilities: {}\\n'.format(preds[0, :5]))\n",
    "    print('Predictions prob/class: ')\n",
    "    \n",
    "    for i in range(len(idxs)):\n",
    "        idx = idxs[i]\n",
    "        print ('  {:.4f}/{}'.format(preds[i, idx], classes[idx]))\n",
    "print(\"method loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "pred_batch(imgs)\n",
    "print(\"--- %s seconds to run this block ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "nbpresent": {
   "slides": {
    "28b43202-5690-4169-9aca-6b9dabfeb3ec": {
     "id": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "prev": null,
     "regions": {
      "3bba644a-cf4d-4a49-9fbd-e2554428cf9f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f3d3a388-7e2a-4151-9b50-c20498fceacc",
        "part": "whole"
       },
       "id": "3bba644a-cf4d-4a49-9fbd-e2554428cf9f"
      }
     }
    },
    "8104def2-4b68-44a0-8f1b-b03bf3b2a079": {
     "id": "8104def2-4b68-44a0-8f1b-b03bf3b2a079",
     "prev": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "regions": {
      "7dded777-1ddf-4100-99ae-25cf1c15b575": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "fe47bd48-3414-4657-92e7-8b8d6cb0df00",
        "part": "whole"
       },
       "id": "7dded777-1ddf-4100-99ae-25cf1c15b575"
      }
     }
    }
   },
   "themes": {}
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
