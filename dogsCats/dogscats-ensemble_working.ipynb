{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#refer week 5 video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5110)\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from theano.sandbox import cuda\n",
    "cuda.use('gpu0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os, sys\n",
    "\n",
    "HOMEPATH = \"/home/ubuntu/fastai/\"\n",
    "\n",
    "os.chdir(HOMEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "import os\n",
    "\n",
    "def set_keras_backend(backend):\n",
    "\n",
    "    if K.backend() != backend:\n",
    "        os.environ['KERAS_BACKEND'] = backend\n",
    "        reload(K)\n",
    "        assert K.backend() == backend\n",
    "\n",
    "set_keras_backend(\"theano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/fastai/data/dogscats/\n",
      "/home/ubuntu/fastai/data/dogscats/models/\n"
     ]
    }
   ],
   "source": [
    "path = HOMEPATH + \"data/dogscats/\"\n",
    "model_path = path + 'models/'\n",
    "print (path)\n",
    "print (model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists(model_path): os.mkdir(model_path)\n",
    "\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(path+'train', shuffle=False, batch_size=batch_size)\n",
    "val_batches = get_batches(path+'valid', shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.preprocessing.image.DirectoryIterator'>\n",
      "<class 'keras.preprocessing.image.DirectoryIterator'>\n"
     ]
    }
   ],
   "source": [
    "print (type(batches))\n",
    "print (type(val_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 12500 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "(val_classes, trn_classes, val_labels, trn_labels, \n",
    "    val_filenames, filenames, test_filenames) = get_classes(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_classes: <type 'numpy.ndarray'> (2000,)\n",
      "trn_classes: <type 'numpy.ndarray'> (23000,)\n",
      "val_labels: <type 'numpy.ndarray'> (2000, 2)\n",
      "trn_labels: <type 'numpy.ndarray'> (23000, 2)\n",
      "val_filenames: <type 'list'> 2000\n",
      "filenames: <type 'list'> 23000\n",
      "test_filenames: <type 'list'> 12500\n"
     ]
    }
   ],
   "source": [
    "print (\"val_classes:\", type(val_classes), val_classes.shape)\n",
    "print (\"trn_classes:\", type(trn_classes), trn_classes.shape)\n",
    "print (\"val_labels:\", type(val_labels), val_labels.shape)\n",
    "print (\"trn_labels:\", type(trn_labels), trn_labels.shape)\n",
    "print (\"val_filenames:\", type(val_filenames), len(val_filenames))\n",
    "print (\"filenames:\", type(filenames), len(filenames))\n",
    "print (\"test_filenames:\", type(test_filenames), len(test_filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we're going to create an ensemble of models and use their average as our predictions. For each ensemble, we're going to follow our usual fine-tuning steps:\n",
    "\n",
    "1) Create a model that retrains just the last layer  \n",
    "2) Add this to a model containing all VGG layers except the last layer  \n",
    "3) Fine-tune just the dense layers of this model (pre-computing the convolutional layers)  \n",
    "4) Add data augmentation, fine-tuning the dense layers without pre-computation.  \n",
    "\n",
    "So first, we need to create our VGG model and pre-compute the output of the conv layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Vgg16().model\n",
    "conv_layers,fc_layers = split_at(model, Convolution2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: <class 'keras.models.Sequential'> <type 'list'> 38\n",
      "conv_layers: <type 'list'> 31\n",
      "fc_layers: <type 'list'> 7\n",
      "<keras.layers.core.Lambda object at 0x7f6ad0f6f250>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad0f6f190>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad0b479d0>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad0b42610>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad0b21150>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f6b0ac75d50>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad0add9d0>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad0a7d150>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad0a8b190>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad0aa18d0>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f6ad0add990>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad0aa7ed0>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad0a44390>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad0a49690>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad0a618d0>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad09ea910>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad09f0d10>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f6ad0aa7e90>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad0a0f3d0>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad0a25b10>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad00aabd0>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad00c1e10>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad00bc950>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad00e2390>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f6ad0a0f390>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad09af910>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad09bd050>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad09d30d0>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad09e65d0>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad096db10>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad0984b10>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f6ad09af8d0>\n",
      "<keras.layers.core.Flatten object at 0x7f6ad0988a90>\n",
      "<keras.layers.core.Dense object at 0x7f6ad08ad090>\n",
      "<keras.layers.core.Dropout object at 0x7f6ad0998750>\n",
      "<keras.layers.core.Dense object at 0x7f6ad084c2d0>\n",
      "<keras.layers.core.Dropout object at 0x7f6ad084ccd0>\n",
      "<keras.layers.core.Dense object at 0x7f6ad0885450>\n"
     ]
    }
   ],
   "source": [
    "print (\"model:\", type(model), type(model.layers), len(model.layers))\n",
    "print (\"conv_layers:\", type(conv_layers), len(conv_layers))\n",
    "print (\"fc_layers:\", type(fc_layers), len(fc_layers))\n",
    "\n",
    "for layer in model.layers:\n",
    "    print (layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model = Sequential(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_model: <class 'keras.models.Sequential'>\n",
      "conv_model: <type 'list'> 31\n",
      "<keras.layers.core.Lambda object at 0x7f6ad0f6f250>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad0f6f190>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad0b479d0>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad0b42610>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad0b21150>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f6b0ac75d50>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad0add9d0>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad0a7d150>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad0a8b190>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad0aa18d0>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f6ad0add990>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad0aa7ed0>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad0a44390>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad0a49690>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad0a618d0>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad09ea910>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad09f0d10>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f6ad0aa7e90>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad0a0f3d0>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad0a25b10>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad00aabd0>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad00c1e10>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad00bc950>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad00e2390>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f6ad0a0f390>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad09af910>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad09bd050>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad09d30d0>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad09e65d0>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad096db10>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad0984b10>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f6ad09af8d0>\n",
      "<keras.layers.core.Flatten object at 0x7f6ad0988a90>\n",
      "<keras.layers.core.Dense object at 0x7f6ad08ad090>\n",
      "<keras.layers.core.Dropout object at 0x7f6ad0998750>\n",
      "<keras.layers.core.Dense object at 0x7f6ad084c2d0>\n",
      "<keras.layers.core.Dropout object at 0x7f6ad084ccd0>\n",
      "<keras.layers.core.Dense object at 0x7f6ad0885450>\n"
     ]
    }
   ],
   "source": [
    "print (\"conv_model:\", type(conv_model))\n",
    "print (\"conv_model:\", type(conv_model.layers), len(conv_model.layers))\n",
    "for layer in model.layers:\n",
    "    print (layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Time elpased (hh:mm:ss.ms) 0:00:50.864010\n",
      "Time elpased (hh:mm:ss.ms) 0:09:04.765339\n"
     ]
    }
   ],
   "source": [
    "print(\"start\")\n",
    "startTime= datetime.now()\n",
    "val_features = conv_model.predict_generator(val_batches, val_batches.nb_sample)\n",
    "timeElapsed=datetime.now()-startTime\n",
    "print('Time elpased (hh:mm:ss.ms) {}'.format(timeElapsed))\n",
    "\n",
    "\n",
    "startTime= datetime.now()\n",
    "trn_features = conv_model.predict_generator(batches, batches.nb_sample)\n",
    "timeElapsed=datetime.now()-startTime\n",
    "print('Time elpased (hh:mm:ss.ms) {}'.format(timeElapsed))\n",
    "\n",
    "#takes long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_features: <type 'numpy.ndarray'>\n",
      "trn_features: <type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print (\"val_features:\", type(val_features))\n",
    "print (\"trn_features:\", type(trn_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_array(model_path + 'train_convlayer_features.bc', trn_features)\n",
    "save_array(model_path + 'valid_convlayer_features.bc', val_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the future we can just load these precomputed features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_features = load_array(model_path+'train_convlayer_features.bc')\n",
    "val_features = load_array(model_path+'valid_convlayer_features.bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_features: <type 'numpy.ndarray'> (2000, 512, 14, 14)\n",
      "trn_features: <type 'numpy.ndarray'> (23000, 512, 14, 14)\n"
     ]
    }
   ],
   "source": [
    "print (\"val_features:\", type(val_features), val_features.shape)\n",
    "print (\"trn_features:\", type(trn_features), trn_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save some time by pre-computing the training and validation arrays with the image decoding and resizing already done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "trn = get_data(path+'train')\n",
    "val = get_data(path+'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_array(model_path+'train_data.bc', trn)\n",
    "save_array(model_path+'valid_data.bc', val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the future we can just load these resized images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn = load_array(model_path+'train_data.bc')\n",
    "val = load_array(model_path+'valid_data.bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print (type(trn))\n",
    "print (type(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can precompute the output of all but the last dropout and dense layers, for creating the first stage of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.models.Sequential'> 38\n",
      "<keras.layers.core.Lambda object at 0x7f6ad0f6f250>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad0f6f190>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad0b479d0>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad0b42610>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad0b21150>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f6b0ac75d50>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad0add9d0>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad0a7d150>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad0a8b190>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad0aa18d0>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f6ad0add990>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad0aa7ed0>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad0a44390>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad0a49690>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad0a618d0>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad09ea910>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad09f0d10>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f6ad0aa7e90>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad0a0f3d0>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad0a25b10>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad00aabd0>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad00c1e10>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad00bc950>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad00e2390>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f6ad0a0f390>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad09af910>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad09bd050>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad09d30d0>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad09e65d0>\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f6ad096db10>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7f6ad0984b10>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f6ad09af8d0>\n",
      "<keras.layers.core.Flatten object at 0x7f6ad0988a90>\n",
      "<keras.layers.core.Dense object at 0x7f6ad08ad090>\n",
      "<keras.layers.core.Dropout object at 0x7f6ad0998750>\n",
      "<keras.layers.core.Dense object at 0x7f6ad084c2d0>\n",
      "<keras.layers.core.Dropout object at 0x7f6ad084ccd0>\n",
      "<keras.layers.core.Dense object at 0x7f6ad0885450>\n"
     ]
    }
   ],
   "source": [
    "print (type(model), len(model.layers))\n",
    "for layer in model.layers:\n",
    "    print (layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pop()\n",
    "model.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_val_feat = model.predict_generator(val_batches, val_batches.nb_sample)\n",
    "ll_feat = model.predict_generator(batches, batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_array(model_path + 'train_ll_feat.bc', ll_feat)\n",
    "save_array(model_path + 'valid_ll_feat.bc', ll_val_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ll_feat: <type 'numpy.ndarray'>\n",
      "ll_val_feat: <type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print (\"ll_feat:\", type(ll_feat))\n",
    "print (\"ll_val_feat:\", type(ll_val_feat))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_feat = load_array(model_path+ 'train_ll_feat.bc')\n",
    "ll_val_feat = load_array(model_path + 'valid_ll_feat.bc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and let's also grab the test data, for when we need to submit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12500 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test = get_data(path+'test')\n",
    "save_array(model_path+'test_data.bc', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_array(model_path+'test_data.bc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions automate creating a model that trains the last layer from scratch, and then adds those new layers on to the main model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ll_layers():\n",
    "    return [ \n",
    "        BatchNormalization(input_shape=(4096,)),\n",
    "        Dropout(0.5),\n",
    "        Dense(2, activation='softmax') \n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_last_layer(i):\n",
    "    ll_layers = get_ll_layers()\n",
    "    ll_model = Sequential(ll_layers)\n",
    "    ll_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    ll_model.optimizer.lr=1e-5\n",
    "    ll_model.fit(ll_feat, trn_labels, validation_data=(ll_val_feat, val_labels), nb_epoch=12)\n",
    "    ll_model.optimizer.lr=1e-7\n",
    "    ll_model.fit(ll_feat, trn_labels, validation_data=(ll_val_feat, val_labels), nb_epoch=1)\n",
    "    ll_model.save_weights(model_path+'ll_bn' + i + '.h5')\n",
    "\n",
    "    vgg = Vgg16()\n",
    "    model = vgg.model\n",
    "    model.pop(); model.pop(); model.pop()\n",
    "    for layer in model.layers: layer.trainable=False\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    ll_layers = get_ll_layers()\n",
    "    for layer in ll_layers: model.add(layer)\n",
    "    for l1,l2 in zip(ll_model.layers, model.layers[-3:]):\n",
    "        l2.set_weights(l1.get_weights())\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.save_weights(model_path+'bn' + i + '.h5')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_model(model):\n",
    "    layers = model.layers\n",
    "    last_conv_idx = [index for index,layer in enumerate(layers) \n",
    "                         if type(layer) is Convolution2D][-1]\n",
    "\n",
    "    conv_layers = layers[:last_conv_idx+1]\n",
    "    conv_model = Sequential(conv_layers)\n",
    "    fc_layers = layers[last_conv_idx+1:]\n",
    "    return conv_model, fc_layers, last_conv_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fc_layers(p, in_shape):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=in_shape),\n",
    "        Flatten(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(4096, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(2, activation='softmax')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dense_layers(i, model):\n",
    "    conv_model, fc_layers, last_conv_idx = get_conv_model(model)\n",
    "    if int(i) > 0:\n",
    "        print 'you info'\n",
    "    conv_shape = conv_model.output_shape[1:]\n",
    "    fc_model = Sequential(get_fc_layers(0.5, conv_shape))\n",
    "    for l1,l2 in zip(fc_model.layers, fc_layers): \n",
    "        weights = l2.get_weights()\n",
    "        l1.set_weights(weights)\n",
    "    fc_model.compile(optimizer=Adam(1e-5), loss='categorical_crossentropy', \n",
    "                     metrics=['accuracy'])\n",
    "    fc_model.fit(trn_features, trn_labels, nb_epoch=2, \n",
    "         batch_size=batch_size, validation_data=(val_features, val_labels))\n",
    "\n",
    "    gen = image.ImageDataGenerator(rotation_range=10, width_shift_range=0.05, \n",
    "       width_zoom_range=0.05, zoom_range=0.05,\n",
    "       channel_shift_range=10, height_shift_range=0.05, shear_range=0.05, horizontal_flip=True)\n",
    "    batches = gen.flow(trn, trn_labels, batch_size=batch_size)\n",
    "    val_batches = image.ImageDataGenerator().flow(val, val_labels, \n",
    "                      shuffle=False, batch_size=batch_size)\n",
    "\n",
    "    for layer in conv_model.layers: layer.trainable = False\n",
    "    for layer in get_fc_layers(0.5, conv_shape): conv_model.add(layer)\n",
    "    for l1,l2 in zip(conv_model.layers[last_conv_idx+1:], fc_model.layers): \n",
    "        l1.set_weights(l2.get_weights())\n",
    "\n",
    "    conv_model.compile(optimizer=Adam(1e-5), loss='categorical_crossentropy', \n",
    "                       metrics=['accuracy'])\n",
    "    conv_model.save_weights(model_path+'no_dropout_bn' + i + '.h5')\n",
    "    conv_model.fit_generator(batches, samples_per_epoch=batches.N, nb_epoch=1, \n",
    "                            validation_data=val_batches, nb_val_samples=val_batches.N)\n",
    "    for layer in conv_model.layers[16:]: layer.trainable = True\n",
    "    conv_model.fit_generator(batches, samples_per_epoch=batches.N, nb_epoch=8, \n",
    "                            validation_data=val_batches, nb_val_samples=val_batches.N)\n",
    "\n",
    "    conv_model.optimizer.lr = 1e-7\n",
    "    conv_model.fit_generator(batches, samples_per_epoch=batches.N, nb_epoch=10, \n",
    "                            validation_data=val_batches, nb_val_samples=val_batches.N)\n",
    "    conv_model.save_weights(model_path + 'aug' + i + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Build ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.7784 - acc: 0.6992 - val_loss: 2.1623 - val_acc: 0.3815\n",
      "Epoch 2/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4855 - acc: 0.8441 - val_loss: 2.6026 - val_acc: 0.3725\n",
      "Epoch 3/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4525 - acc: 0.8658 - val_loss: 2.7598 - val_acc: 0.3690\n",
      "Epoch 4/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4335 - acc: 0.8743 - val_loss: 2.8023 - val_acc: 0.3650\n",
      "Epoch 5/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4256 - acc: 0.8791 - val_loss: 2.7783 - val_acc: 0.3635\n",
      "Epoch 6/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4174 - acc: 0.8808 - val_loss: 2.7921 - val_acc: 0.3645\n",
      "Epoch 7/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4049 - acc: 0.8857 - val_loss: 2.7961 - val_acc: 0.3650\n",
      "Epoch 8/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.4025 - acc: 0.8851 - val_loss: 2.7454 - val_acc: 0.3630\n",
      "Epoch 9/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.3926 - acc: 0.8877 - val_loss: 2.7011 - val_acc: 0.3635\n",
      "Epoch 10/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.3890 - acc: 0.8893 - val_loss: 2.6679 - val_acc: 0.3640\n",
      "Epoch 11/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.3837 - acc: 0.8896 - val_loss: 2.6933 - val_acc: 0.3635\n",
      "Epoch 12/12\n",
      "23000/23000 [==============================] - 3s - loss: 0.3738 - acc: 0.8916 - val_loss: 2.6300 - val_acc: 0.3625\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 3s - loss: 0.3779 - acc: 0.8893 - val_loss: 2.6346 - val_acc: 0.3620\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You called `set_weights(weights)` on layer \"batchnormalization_3\" with a  weight list of length 0, but the layer was expecting 4 weights. Provided weights: []...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-6d8fda4e55b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_last_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_dense_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-109a0a90fc94>\u001b[0m in \u001b[0;36mtrain_dense_layers\u001b[0;34m(i, model)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0ml1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     fc_model.compile(optimizer=Adam(1e-5), loss='categorical_crossentropy', \n\u001b[1;32m      9\u001b[0m                      metrics=['accuracy'])\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m    973\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m                              \u001b[0;34m' weights. Provided weights: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m                              str(weights)[:50] + '...')\n\u001b[0m\u001b[1;32m    976\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You called `set_weights(weights)` on layer \"batchnormalization_3\" with a  weight list of length 0, but the layer was expecting 4 weights. Provided weights: []..."
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    i = str(i)\n",
    "    model = train_last_layer(i)\n",
    "    if int(i) > 0:\n",
    "        model.summary()\n",
    "    train_dense_layers(i, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine ensemble and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_model = vgg_ft(2)\n",
    "for layer in ens_model.layers: layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ens_pred(arr, fname):\n",
    "    ens_pred = []\n",
    "    for i in range(5):\n",
    "        i = str(i)\n",
    "        ens_model.load_weights('{}{}{}.h5'.format(model_path, fname, i))\n",
    "        preds = ens_model.predict(arr, batch_size=batch_size)\n",
    "        ens_pred.append(preds)\n",
    "    return ens_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred2 = get_ens_pred(val, 'aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_avg_preds2 = np.stack(val_pred2).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_accuracy(val_labels, val_avg_preds2).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
