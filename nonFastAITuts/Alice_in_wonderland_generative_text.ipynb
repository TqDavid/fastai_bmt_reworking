{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawn from https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small LSTM Network to Generate Text for Alice in Wonderland\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"http://www.gutenberg.org/cache/epub/11/pg11.txt\"\n",
    "#scp -i /c/blah/.ssh/blah.pem /d/blah/alice_in_wonderland_11-0.txt ubuntu@blah:~/blah/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"alice_in_wonderland_11-0.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'str'> 173595\n"
     ]
    }
   ],
   "source": [
    "print type(raw_text), len(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  173595\n",
      "Total Vocab:  66\n"
     ]
    }
   ],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print \"Total Characters: \", n_chars\n",
    "print \"Total Vocab: \", n_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  173595\n",
      "Total Vocab:  66\n"
     ]
    }
   ],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print \"Total Characters: \", n_chars\n",
    "print \"Total Vocab: \", n_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  173495\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print \"Total Patterns: \", n_patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(173495, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'> (173495, 65)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "print type(y), y.shape\n",
    "print y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lstm_4 (LSTM)                    (None, 256)           264192      lstm_input_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 256)           0           lstm_4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 65)            16705       dropout_4[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 280,897\n",
      "Trainable params: 280,897\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "print model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "#https://keras.io/callbacks/\n",
    "#Save the model after every epoch.\n",
    "callbacks_list = [checkpoint]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('started at ', datetime.datetime(2017, 11, 21, 22, 58, 37, 567207))\n",
      "Epoch 1/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.9848Epoch 00000: loss improved from inf to 2.98470, saving model to weights-improvement-00-2.9847.hdf5\n",
      "173495/173495 [==============================] - 197s - loss: 2.9847   \n",
      "Epoch 2/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.6913Epoch 00001: loss improved from 2.98470 to 2.69127, saving model to weights-improvement-01-2.6913.hdf5\n",
      "173495/173495 [==============================] - 196s - loss: 2.6913   \n",
      "Epoch 3/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.5832Epoch 00002: loss improved from 2.69127 to 2.58314, saving model to weights-improvement-02-2.5831.hdf5\n",
      "173495/173495 [==============================] - 193s - loss: 2.5831   \n",
      "Epoch 4/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.5150Epoch 00003: loss improved from 2.58314 to 2.51495, saving model to weights-improvement-03-2.5150.hdf5\n",
      "173495/173495 [==============================] - 190s - loss: 2.5150   \n",
      "Epoch 5/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.4582Epoch 00004: loss improved from 2.51495 to 2.45828, saving model to weights-improvement-04-2.4583.hdf5\n",
      "173495/173495 [==============================] - 189s - loss: 2.4583   \n",
      "Epoch 6/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.4011Epoch 00005: loss improved from 2.45828 to 2.40111, saving model to weights-improvement-05-2.4011.hdf5\n",
      "173495/173495 [==============================] - 189s - loss: 2.4011   \n",
      "Epoch 7/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.3565Epoch 00006: loss improved from 2.40111 to 2.35655, saving model to weights-improvement-06-2.3566.hdf5\n",
      "173495/173495 [==============================] - 189s - loss: 2.3566   \n",
      "Epoch 8/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.3149Epoch 00007: loss improved from 2.35655 to 2.31500, saving model to weights-improvement-07-2.3150.hdf5\n",
      "173495/173495 [==============================] - 189s - loss: 2.3150   \n",
      "Epoch 9/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.2794Epoch 00008: loss improved from 2.31500 to 2.27953, saving model to weights-improvement-08-2.2795.hdf5\n",
      "173495/173495 [==============================] - 189s - loss: 2.2795   \n",
      "Epoch 10/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.2465Epoch 00009: loss improved from 2.27953 to 2.24650, saving model to weights-improvement-09-2.2465.hdf5\n",
      "173495/173495 [==============================] - 189s - loss: 2.2465   \n",
      "Epoch 11/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.2157Epoch 00010: loss improved from 2.24650 to 2.21570, saving model to weights-improvement-10-2.2157.hdf5\n",
      "173495/173495 [==============================] - 189s - loss: 2.2157   \n",
      "Epoch 12/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.1851Epoch 00011: loss improved from 2.21570 to 2.18499, saving model to weights-improvement-11-2.1850.hdf5\n",
      "173495/173495 [==============================] - 190s - loss: 2.1850   \n",
      "Epoch 13/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.1572Epoch 00012: loss improved from 2.18499 to 2.15713, saving model to weights-improvement-12-2.1571.hdf5\n",
      "173495/173495 [==============================] - 207s - loss: 2.1571   \n",
      "Epoch 14/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.1313Epoch 00013: loss improved from 2.15713 to 2.13128, saving model to weights-improvement-13-2.1313.hdf5\n",
      "173495/173495 [==============================] - 192s - loss: 2.1313   \n",
      "Epoch 15/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.1078Epoch 00014: loss improved from 2.13128 to 2.10778, saving model to weights-improvement-14-2.1078.hdf5\n",
      "173495/173495 [==============================] - 190s - loss: 2.1078   \n",
      "Epoch 16/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.0849Epoch 00015: loss improved from 2.10778 to 2.08484, saving model to weights-improvement-15-2.0848.hdf5\n",
      "173495/173495 [==============================] - 191s - loss: 2.0848   \n",
      "Epoch 17/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.0625Epoch 00016: loss improved from 2.08484 to 2.06260, saving model to weights-improvement-16-2.0626.hdf5\n",
      "173495/173495 [==============================] - 192s - loss: 2.0626   \n",
      "Epoch 18/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.0421Epoch 00017: loss improved from 2.06260 to 2.04219, saving model to weights-improvement-17-2.0422.hdf5\n",
      "173495/173495 [==============================] - 191s - loss: 2.0422   \n",
      "Epoch 19/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.0207Epoch 00018: loss improved from 2.04219 to 2.02066, saving model to weights-improvement-18-2.0207.hdf5\n",
      "173495/173495 [==============================] - 190s - loss: 2.0207   \n",
      "Epoch 20/20\n",
      "173440/173495 [============================>.] - ETA: 0s - loss: 2.0013Epoch 00019: loss improved from 2.02066 to 2.00143, saving model to weights-improvement-19-2.0014.hdf5\n",
      "173495/173495 [==============================] - 191s - loss: 2.0014   \n",
      "Time elapsed (hh:mm:ss.ms) 1:04:05.481552\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "startTime= datetime.now()\n",
    "print (\"started at \", startTime)\n",
    "model.fit(X, y, nb_epoch=20, batch_size=128, callbacks=callbacks_list)\n",
    "timeElapsed=datetime.now()-startTime\n",
    "print('Time elapsed (hh:mm:ss.ms) {}'.format(timeElapsed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
