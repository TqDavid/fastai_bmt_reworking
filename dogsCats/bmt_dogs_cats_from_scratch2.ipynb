{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current dir: /home/ubuntu/fastai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5110)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division,print_function\n",
    "import os, sys\n",
    "os.chdir(\"/home/ubuntu/fastai/\")\n",
    "print(\"current dir:\", os.getcwd() )\n",
    "#import modules\n",
    "from utils import *\n",
    "from vgg16 import Vgg16\n",
    "\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, linewidth=100)\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#Instantiate plotting tool\n",
    "#In Jupyter notebooks, you will need to run this command before doing any plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current dir: /home/ubuntu/fastai/data/dogscats\n",
      "test_path: /home/ubuntu/fastai/data/dogscats/test1/\n",
      "results_path: /home/ubuntu/fastai/data/dogscats/results/\n",
      "train_path: /home/ubuntu/fastai/data/dogscats/train/\n",
      "valid_path: /home/ubuntu/fastai/data/dogscats/valid/\n"
     ]
    }
   ],
   "source": [
    "#Create references to important directories we will use over and over\n",
    "DATA_HOME_DIR = \"/home/ubuntu/fastai/data/dogscats/\"\n",
    "os.chdir(DATA_HOME_DIR)\n",
    "print(\"current dir:\", os.getcwd() )\n",
    "\n",
    "#Set path to sample/ path if desired\n",
    "test_path = DATA_HOME_DIR + 'test1/' #We use all the test data\n",
    "results_path=DATA_HOME_DIR + 'results/'\n",
    "train_path=DATA_HOME_DIR + 'train/'\n",
    "valid_path=DATA_HOME_DIR + 'valid/'\n",
    "print (\"test_path:\", test_path)\n",
    "print (\"results_path:\", results_path)\n",
    "print (\"train_path:\", train_path)\n",
    "print (\"valid_path:\", valid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As large as you can, but no larger than 64 is recommended. \n",
    "# If you have an older or cheaper GPU, you'll run out of memory, so will have to decrease this.\n",
    "batch_size=64\n",
    "no_of_epochs=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = Vgg16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#Finetune the model\n",
    "print (\"start\")\n",
    "batches = vgg.get_batches(train_path, batch_size=batch_size)\n",
    "val_batches = vgg.get_batches(valid_path, batch_size=batch_size*2)\n",
    "vgg.finetune(batches)\n",
    "\n",
    "#Not sure if we set this for all fits\n",
    "vgg.model.optimizer.lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch: 0\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 590s - loss: 0.3537 - acc: 0.9695 - val_loss: 0.2283 - val_acc: 0.9820\n",
      "Running epoch: 1\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 601s - loss: 0.3098 - acc: 0.9770 - val_loss: 0.3177 - val_acc: 0.9775\n",
      "Running epoch: 2\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 600s - loss: 0.3381 - acc: 0.9763 - val_loss: 0.2697 - val_acc: 0.9800\n",
      "Completed 3 fit operations\n"
     ]
    }
   ],
   "source": [
    "#Notice we are passing in the validation dataset to the fit() method\n",
    "#For each epoch we test our model against the validation set\n",
    "#latest_weights_filename = None\n",
    "for epoch in range(no_of_epochs):\n",
    "    print (\"Running epoch: %d\" % epoch)\n",
    "    vgg.fit(batches, val_batches, nb_epoch=1)\n",
    "    #latest_weights_filename = 'ft%d.h5' % epoch\n",
    "    #vgg.model.save_weights(results_path+latest_weights_filename)\n",
    "print (\"Completed %s fit operations\" % no_of_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Found 12500 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "print (\"start\")\n",
    "batches, preds = vgg.test(test_path, batch_size = batch_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "val_batches, probs = vgg.test(valid_path, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = val_batches.filenames\n",
    "expected_labels = val_batches.classes #0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12500, 2)\n",
      "[ 1.  0.  0.  1.  1.  0.  1.  1.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print (preds.shape)\n",
    "print (preds[0:10, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Round our predictions to 0/1 to generate labels\n",
    "our_predictions = probs[:,0]\n",
    "our_labels = np.round(1-our_predictions)\n",
    "#NB: later we adjust this method to 0.05 and 0.95 for better scoring in kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12500,)\n",
      "our_predictions: [ 1.  0.  0.  1.  1.  0.  1.  1.  0.  0.]\n",
      "our_labels: [ 0.  1.  1.  0.  0.  1.  0.  0.  1.  1.]\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "print (our_predictions.shape)\n",
    "print (\"our_predictions:\", our_predictions[0:10])\n",
    "print (\"our_labels:\", our_labels[0:10])\n",
    "print (expected_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 correct labels\n",
      "Found 1 incorrect labels\n",
      "Found 0 confident correct cats labels\n",
      "Found 0 confident correct dogs labels\n",
      "Found 6126 incorrect cats\n",
      "Found 6374 incorrect dogs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  \"\"\"\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  import sys\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n",
      "  if __name__ == '__main__':\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: elementwise != comparison failed; this will raise an error in the future.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "correct = np.where(our_labels==expected_labels)[0]\n",
    "print (\"Found %d correct labels\" % len(correct))\n",
    "incorrect = np.where(our_labels!=expected_labels)[0]\n",
    "print (\"Found %d incorrect labels\" % len(incorrect))\n",
    "correct_cats = np.where((our_labels==0) & (our_labels==expected_labels))[0]\n",
    "print (\"Found %d confident correct cats labels\" % len(correct_cats))\n",
    "correct_dogs = np.where((our_labels==1) & (our_labels==expected_labels))[0]\n",
    "print (\"Found %d confident correct dogs labels\" % len(correct_dogs))\n",
    "incorrect_cats = np.where((our_labels==0) & (our_labels!=expected_labels))[0]\n",
    "print (\"Found %d incorrect cats\" % len(incorrect_cats))\n",
    "incorrect_dogs = np.where((our_labels==1) & (our_labels!=expected_labels))[0]\n",
    "print (\"Found %d incorrect dogs\" % len(incorrect_dogs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(expected_labels, our_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm, val_batches.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grab the dog prediction column\n",
    "isdog = preds[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract imageIds from the filenames in our test/unknown directory \n",
    "filenames = batches.filenames\n",
    "ids = np.array([int(f[8:f.find('.')]) for f in filenames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm = np.stack([ids,isdog], axis=1)\n",
    "subm[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file_name = 'submission2.csv'\n",
    "print (\"submission_file_name:\", submission_file_name)\n",
    "np.savetxt(results_path+submission_file_name, subm, fmt='%d,%.5f', header='id,label', comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "%cd $results_path\n",
    "FileLink(submission_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
